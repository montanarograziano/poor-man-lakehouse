{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "308570c7",
   "metadata": {},
   "source": [
    "# Experimenting with PyIceberg\n",
    "\n",
    "This notebooks serves as experimenting snippes with PyIceberg capabilities. It will include:\n",
    "\n",
    "- Read operations\n",
    "- Write operations (MERGE, write Hive partitioned, partition overwrite etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0996cc3f",
   "metadata": {},
   "source": [
    "For demo purposes, we'll follow the example provided in the documentation [here](https://py.iceberg.apache.org/api/), so we'll create a local catalog, some tables and perform operations.\n",
    "\n",
    "Everytime the notebook is executed, we'll recreate the warehouse folder so that we don't get errors in `load_catalog` function (CREATE IF NOT EXISTS seems not to be supported)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f280c927",
   "metadata": {},
   "source": [
    "## Connect to Nessie Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f50a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import UTC, datetime\n",
    "\n",
    "from pyiceberg.catalog import load_catalog\n",
    "\n",
    "catalog_config = {\n",
    "    \"type\": \"sql\",\n",
    "    \"uri\": \"postgresql+psycopg2://user:password@postgres_db/catalog_db\",\n",
    "}\n",
    "catalog = load_catalog(\"sql_catalog\", **catalog_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3f8c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(catalog.list_namespaces())\n",
    "if (\"default\",) not in catalog.list_namespaces():\n",
    "    catalog.create_namespace(\"default\")\n",
    "\n",
    "ns = catalog.list_namespaces()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d1a745",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog.list_tables(\"default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85a80fc",
   "metadata": {},
   "source": [
    "## Create a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3aaa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pyarrow as pa\n",
    "from pyiceberg.partitioning import PartitionField, PartitionSpec\n",
    "from pyiceberg.schema import Schema\n",
    "from pyiceberg.table.sorting import SortField, SortOrder\n",
    "from pyiceberg.transforms import DayTransform, IdentityTransform\n",
    "from pyiceberg.types import (\n",
    "    DoubleType,\n",
    "    FloatType,\n",
    "    NestedField,\n",
    "    StringType,\n",
    "    StructType,\n",
    "    TimestampType,\n",
    ")\n",
    "\n",
    "schema = Schema(\n",
    "    NestedField(field_id=1, name=\"datetime\", field_type=TimestampType(), required=True),\n",
    "    NestedField(field_id=2, name=\"symbol\", field_type=StringType(), required=True),\n",
    "    NestedField(field_id=3, name=\"bid\", field_type=FloatType(), required=False),\n",
    "    NestedField(field_id=4, name=\"ask\", field_type=DoubleType(), required=False),\n",
    "    NestedField(\n",
    "        field_id=5,\n",
    "        name=\"details\",\n",
    "        field_type=StructType(\n",
    "            NestedField(field_id=4, name=\"created_by\", field_type=StringType(), required=False),\n",
    "        ),\n",
    "        required=False,\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "partition_spec = PartitionSpec(\n",
    "    PartitionField(source_id=1, field_id=1000, transform=DayTransform(), name=\"datetime_day\")\n",
    ")\n",
    "\n",
    "\n",
    "# Sort on the symbol\n",
    "sort_order = SortOrder(SortField(source_id=2, transform=IdentityTransform()))\n",
    "\n",
    "if not catalog.table_exists(\"default.bids\"):\n",
    "    catalog.create_table(\n",
    "        identifier=\"default.bids\",\n",
    "        schema=schema,\n",
    "        location=\"s3://warehouse/bids\",\n",
    "        partition_spec=partition_spec,\n",
    "        sort_order=sort_order,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c013f259",
   "metadata": {},
   "source": [
    "## Load a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93f0929",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = catalog.load_table(\"default.bids\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb349fc",
   "metadata": {},
   "source": [
    "## Check if table exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cd69ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog.table_exists(\"default.bids\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936abfc3",
   "metadata": {},
   "source": [
    "## Convert to Polars DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e91dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_df = table.scan().to_polars()\n",
    "table_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96f6fcb",
   "metadata": {},
   "source": [
    "## Append some data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cdae9c",
   "metadata": {},
   "source": [
    "Note that is necessary to convert the pyarrow schema to the one of the table, not the one inferred by Polars.\n",
    "\n",
    "This is because Polars does not care about not null columns, therefore Iceberg will give errors due to schema missmatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dec2523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_to_pyarrow(df: pl.DataFrame, schema: pa.Schema) -> pa.Table:\n",
    "    \"\"\"Cast a Polars DataFrame to a PyArrow Table with the given schema.\"\"\"\n",
    "    arrow_df = df.to_arrow()\n",
    "    return arrow_df.cast(schema)\n",
    "\n",
    "\n",
    "data = pl.DataFrame(\n",
    "    {\n",
    "        \"datetime\": [\n",
    "            datetime(2023, 1, 1, 12, 0, tzinfo=UTC),\n",
    "            datetime(2023, 1, 2, 12, 0, tzinfo=UTC),\n",
    "            datetime(2023, 1, 3, 12, 0, tzinfo=UTC),\n",
    "        ],\n",
    "        \"symbol\": [\"AAPL\", \"GOOGL\", \"MSFT\"],\n",
    "        \"bid\": [150.0, 2800.0, 300.0],\n",
    "        \"ask\": [151.0, 2805.0, 305.0],\n",
    "        \"details\": [\n",
    "            {\"created_by\": \"user1\"},\n",
    "            {\"created_by\": \"user2\"},\n",
    "            {\"created_by\": None},\n",
    "        ],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d174f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "table.append(df=cast_to_pyarrow(data, table.schema().as_arrow()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aad328d",
   "metadata": {},
   "outputs": [],
   "source": [
    "table.scan().to_polars()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a44180",
   "metadata": {},
   "source": [
    "## Test upsert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e123f338",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pa.Table.from_pylist(\n",
    "    [\n",
    "        {\n",
    "            \"datetime\": datetime(2023, 1, 1, 12, 0, tzinfo=UTC),\n",
    "            \"symbol\": \"AAPL2\",\n",
    "            \"bid\": 150.0,\n",
    "            \"ask\": 151.0,\n",
    "            \"details\": {\"created_by\": \"user1\"},\n",
    "        },\n",
    "        {\n",
    "            \"datetime\": datetime(2023, 1, 4, 12, 0, tzinfo=UTC),\n",
    "            \"symbol\": \"AMZ\",\n",
    "            \"bid\": 2800.0,\n",
    "            \"ask\": 2805.0,\n",
    "            \"details\": {\"created_by\": \"user1\"},\n",
    "        },\n",
    "    ],\n",
    "    schema=table.schema().as_arrow(),\n",
    ")\n",
    "pl.from_arrow(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507e48a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "table.upsert(df, join_cols=[\"datetime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f92342f",
   "metadata": {},
   "outputs": [],
   "source": [
    "table.scan().to_polars().sort(\"datetime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7ebe02",
   "metadata": {},
   "source": [
    "# Check partition overwrite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d03ea65",
   "metadata": {},
   "source": [
    "Here we'll check if partial overwrite does work. We'll use the same dataframe as before for the upsert, but we'll run an `overwrite` operation only on the partition `datetime=2023-01-01T12:00:00`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec26ab20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyiceberg.expressions import EqualTo\n",
    "\n",
    "df = pa.Table.from_pylist(\n",
    "    [\n",
    "        {\n",
    "            \"datetime\": datetime(2023, 1, 1, 12, 0, tzinfo=UTC),\n",
    "            \"symbol\": \"AAPL2\",\n",
    "            \"bid\": 150.0,\n",
    "            \"ask\": 151.0,\n",
    "            \"details\": {\"created_by\": \"user1\"},\n",
    "        },\n",
    "        {\n",
    "            \"datetime\": datetime(2023, 1, 4, 12, 0, tzinfo=UTC),\n",
    "            \"symbol\": \"AMZ\",\n",
    "            \"bid\": 2800.0,\n",
    "            \"ask\": 2805.0,\n",
    "            \"details\": {\"created_by\": \"user1\"},\n",
    "        },\n",
    "    ],\n",
    "    schema=table.schema().as_arrow(),\n",
    ")\n",
    "pl.from_arrow(df)\n",
    "print(pl.from_arrow(df))\n",
    "table.overwrite(df, overwrite_filter=EqualTo(\"datetime\", \"2023-01-01T12:00:00\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6186706d",
   "metadata": {},
   "outputs": [],
   "source": [
    "table.scan().to_polars()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b5b6d1",
   "metadata": {},
   "source": [
    "This is cool, because it respected the filter, and it created a duplicate for ` 2023-01-04 12:00:00` since it was not specified in the filter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370faed0",
   "metadata": {},
   "source": [
    "## Overwrite from Polars\n",
    "\n",
    "Polars has `read_iceberg` and `write_iceberg` predicates, let's see if they work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc481d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.scan_iceberg(table, reader_override=\"pyiceberg\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d21ddb",
   "metadata": {},
   "source": [
    "There are still problems with how schema is evaluated for required fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34717809",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.write_iceberg(table, \"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e258802",
   "metadata": {},
   "source": [
    "## Partition evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73299036",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyiceberg.transforms import IdentityTransform\n",
    "\n",
    "with table.update_spec() as update:\n",
    "    # update.add_field(\"symbol\", IdentityTransform(), \"symbol\")\n",
    "    update.rename_field(\"datetime_day\", \"datetime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36e49c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "table.inspect.partitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9162d5e9",
   "metadata": {},
   "source": [
    "## Check History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151f1c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.from_arrow(table.inspect.history())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poor-man-lakehouse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
