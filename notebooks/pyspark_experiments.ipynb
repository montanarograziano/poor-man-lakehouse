{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea60e9e",
   "metadata": {},
   "outputs": [],
   "source": "from poor_man_lakehouse.config import settings\nfrom poor_man_lakehouse.spark_connector.builder import retrieve_current_spark_session\n\nspark = retrieve_current_spark_session()"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31997cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|      catalog|\n",
      "+-------------+\n",
      "|     postgres|\n",
      "|spark_catalog|\n",
      "+-------------+\n",
      "\n",
      "+----------+\n",
      "| namespace|\n",
      "+----------+\n",
      "|   default|\n",
      "|pg_catalog|\n",
      "+----------+\n",
      "\n",
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n",
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|  default|    prova|      false|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW CATALOGS\").show()\n",
    "spark.sql(f\"show schemas in {settings.CATALOG}\").show()\n",
    "spark.sql(\"show schemas in spark_catalog\").show()\n",
    "spark.sql(f\"SHOW  tables in {settings.CATALOG}.default\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feca4502",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import UTC, datetime\n",
    "\n",
    "import polars as pl\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "data = pl.DataFrame(\n",
    "    {\n",
    "        \"datetime\": [\n",
    "            datetime(2023, 1, 1, 12, 0, tzinfo=UTC),\n",
    "            datetime(2023, 1, 2, 12, 0, tzinfo=UTC),\n",
    "            datetime(2023, 1, 3, 12, 0, tzinfo=UTC),\n",
    "        ],\n",
    "        \"symbol\": [\"AAPL\", \"GOOGL\", \"MSFT\"],\n",
    "        \"bid\": [150.0, 2800.0, 300.0],\n",
    "        \"ask\": [151.0, 2805.0, 305.0],\n",
    "        \"details\": [\n",
    "            {\"created_by\": \"user1\"},\n",
    "            {\"created_by\": \"user2\"},\n",
    "            {\"created_by\": None},\n",
    "        ],\n",
    "    },\n",
    ")\n",
    "\n",
    "spark_df: DataFrame = spark.createDataFrame(data.to_pandas())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2825702c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/13 23:31:33 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark_df.write.format(\"iceberg\").mode(\"overwrite\").saveAsTable(\n",
    "    f\"{settings.CATALOG}.default.prova\", mode=\"overwrite\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5617251f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+------+------+--------------------+\n",
      "|           datetime|symbol|   bid|   ask|             details|\n",
      "+-------------------+------+------+------+--------------------+\n",
      "|2023-01-01 12:00:00|  AAPL| 150.0| 151.0|{created_by -> us...|\n",
      "|2023-01-02 12:00:00| GOOGL|2800.0|2805.0|{created_by -> us...|\n",
      "|2023-01-03 12:00:00|  MSFT| 300.0| 305.0|{created_by -> NULL}|\n",
      "+-------------------+------+------+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"\"\"SELECT * FROM {settings.CATALOG}.default.prova\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc070cc",
   "metadata": {},
   "source": [
    "## Testing LakeSail\n",
    "\n",
    "To test Lakesail, it's necessary to restart the kernel since the sparkSession was already created and we need a clean one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86483ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysail.spark import SparkConnectServer\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "server = SparkConnectServer()\n",
    "server.start()\n",
    "address = server.listening_address\n",
    "if address is None:\n",
    "    raise RuntimeError(\"Failed to start Spark Connect server\")\n",
    "_, port = address\n",
    "\n",
    "spark = SparkSession.builder.remote(f\"sc://localhost:{port}\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918bceba",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"s3a://warehouse/test_parquet\"\n",
    "df = spark.createDataFrame([(1, \"Alice\"), (2, \"Bob\")], schema=\"id INT, name STRING\")\n",
    "df.write.parquet(path)\n",
    "\n",
    "df = spark.read.parquet(path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poor-man-lakehouse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}