{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33010be7",
   "metadata": {},
   "source": [
    "# Notebook for Ibis experiments\n",
    "\n",
    "Please note that this notebook require the table *prova*, which can be created inside the *pyspark_experiments.ipynb* notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c4f0f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "26/02/07 19:24:57 WARN Utils: Your hostname, MBP-di-Graziano.homenet.telecomitalia.it, resolves to a loopback address: 127.0.0.1; using 192.168.1.81 instead (on interface en0)\n",
      "26/02/07 19:24:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/Users/graziano/apache-spark/4.0.1/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /Users/graziano/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /Users/graziano/.ivy2.5.2/jars\n",
      "io.delta#delta-spark_2.13 added as a dependency\n",
      "org.apache.iceberg#iceberg-spark-runtime-4.0_2.13 added as a dependency\n",
      "org.apache.iceberg#iceberg-aws-bundle added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "org.postgresql#postgresql added as a dependency\n",
      "org.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.13 added as a dependency\n",
      "io.unitycatalog#unitycatalog-spark_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-e48f446d-958e-4717-a5ee-c6d3621f891f;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.13;4.0.0 in central\n",
      "\tfound io.delta#delta-storage;4.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.13.1 in central\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-4.0_2.13;1.10.1 in central\n",
      "\tfound org.apache.iceberg#iceberg-aws-bundle;1.10.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.4.0 in central\n",
      "\tfound software.amazon.awssdk#bundle;2.23.19 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.1.3.Final in central\n",
      "\tfound org.postgresql#postgresql;42.7.3 in central\n",
      "\tfound org.checkerframework#checker-qual;3.42.0 in central\n",
      "\tfound org.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.13;0.106.0 in central\n",
      "\tfound io.unitycatalog#unitycatalog-spark_2.13;0.3.1 in central\n",
      "\tfound io.unitycatalog#unitycatalog-client;0.3.1 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.13 in central\n",
      "\tfound org.apache.logging.log4j#log4j-slf4j2-impl;2.24.3 in central\n",
      "\tfound org.apache.logging.log4j#log4j-api;2.24.3 in central\n",
      "\tfound org.apache.logging.log4j#log4j-core;2.24.3 in central\n",
      "\tfound com.fasterxml.jackson.datatype#jackson-datatype-jsr310;2.17.0 in central\n",
      "\tfound org.openapitools#jackson-databind-nullable;0.2.6 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-databind;2.15.0 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-annotations;2.15.0 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.15.0 in central\n",
      "\tfound com.fasterxml.jackson.module#jackson-module-scala_2.13;2.15.0 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.8 in central\n",
      "\tfound com.fasterxml.jackson.dataformat#jackson-dataformat-xml;2.15.0 in central\n",
      "\tfound org.codehaus.woodstox#stax2-api;4.2.1 in central\n",
      "\tfound com.fasterxml.woodstox#woodstox-core;6.5.1 in central\n",
      "\tfound org.antlr#antlr4;4.13.1 in central\n",
      "\tfound org.antlr#antlr-runtime;3.5.3 in central\n",
      "\tfound org.antlr#ST4;4.3.4 in central\n",
      "\tfound org.abego.treelayout#org.abego.treelayout.core;1.0.3 in central\n",
      "\tfound com.ibm.icu#icu4j;72.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.4.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.4.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.4 in central\n",
      "\tfound commons-logging#commons-logging;1.2 in central\n",
      ":: resolution report :: resolve 519ms :: artifacts dl 11ms\n",
      "\t:: modules in use:\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.15.0 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.15.0 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.15.0 from central in [default]\n",
      "\tcom.fasterxml.jackson.dataformat#jackson-dataformat-xml;2.15.0 from central in [default]\n",
      "\tcom.fasterxml.jackson.datatype#jackson-datatype-jsr310;2.17.0 from central in [default]\n",
      "\tcom.fasterxml.jackson.module#jackson-module-scala_2.13;2.15.0 from central in [default]\n",
      "\tcom.fasterxml.woodstox#woodstox-core;6.5.1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.ibm.icu#icu4j;72.1 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.8 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.2 from central in [default]\n",
      "\tio.delta#delta-spark_2.13;4.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;4.0.0 from central in [default]\n",
      "\tio.unitycatalog#unitycatalog-client;0.3.1 from central in [default]\n",
      "\tio.unitycatalog#unitycatalog-spark_2.13;0.3.1 from central in [default]\n",
      "\torg.abego.treelayout#org.abego.treelayout.core;1.0.3 from central in [default]\n",
      "\torg.antlr#ST4;4.3.4 from central in [default]\n",
      "\torg.antlr#antlr-runtime;3.5.3 from central in [default]\n",
      "\torg.antlr#antlr4;4.13.1 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.13.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.4.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.4.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.4.0 from central in [default]\n",
      "\torg.apache.iceberg#iceberg-aws-bundle;1.10.1 from central in [default]\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-4.0_2.13;1.10.1 from central in [default]\n",
      "\torg.apache.logging.log4j#log4j-api;2.24.3 from central in [default]\n",
      "\torg.apache.logging.log4j#log4j-core;2.24.3 from central in [default]\n",
      "\torg.apache.logging.log4j#log4j-slf4j2-impl;2.24.3 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.42.0 from central in [default]\n",
      "\torg.codehaus.woodstox#stax2-api;4.2.1 from central in [default]\n",
      "\torg.openapitools#jackson-databind-nullable;0.2.6 from central in [default]\n",
      "\torg.postgresql#postgresql;42.7.3 from central in [default]\n",
      "\torg.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.13;0.106.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.13 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.1.3.Final from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.4 from central in [default]\n",
      "\tsoftware.amazon.awssdk#bundle;2.23.19 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.slf4j#slf4j-api;2.0.16 by [org.slf4j#slf4j-api;2.0.13] in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.17.0 by [com.fasterxml.jackson.core#jackson-annotations;2.15.0] in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.17.0 by [com.fasterxml.jackson.core#jackson-core;2.15.0] in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.17.0 by [com.fasterxml.jackson.core#jackson-databind;2.15.0] in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.14.0-rc2 by [com.fasterxml.jackson.core#jackson-databind;2.15.0] in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.36 by [org.slf4j#slf4j-api;2.0.13] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   43  |   0   |   0   |   6   ||   37  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-e48f446d-958e-4717-a5ee-c6d3621f891f\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 37 already retrieved (0kB/5ms)\n",
      "26/02/07 19:24:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "\u001b[32m2026-02-07 19:25:04.279\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpoor_man_lakehouse.dremio_connector.builder\u001b[0m:\u001b[36m_initialize_dremio\u001b[0m:\u001b[36m150\u001b[0m - \u001b[1mInitializing Dremio setup...\u001b[0m\n",
      "\u001b[32m2026-02-07 19:25:04.688\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpoor_man_lakehouse.dremio_connector.builder\u001b[0m:\u001b[36m_initialize_dremio\u001b[0m:\u001b[36m155\u001b[0m - \u001b[1mAdmin user authentication successful\u001b[0m\n",
      "\u001b[32m2026-02-07 19:25:05.007\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpoor_man_lakehouse.dremio_connector.builder\u001b[0m:\u001b[36m_ensure_nessie_catalog\u001b[0m:\u001b[36m245\u001b[0m - \u001b[1mNessie catalog 'nessie' already exists\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import ibis\n",
    "import polars as pl\n",
    "\n",
    "from poor_man_lakehouse.dremio_connector.builder import DremioConnection\n",
    "from poor_man_lakehouse.ibis_connector.builder import IbisConnection\n",
    "from poor_man_lakehouse.spark_connector.builder import retrieve_current_spark_session\n",
    "\n",
    "spark = retrieve_current_spark_session()\n",
    "\n",
    "ibis.options.interactive = True\n",
    "\n",
    "d = DremioConnection()\n",
    "conn = IbisConnection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e912ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lakekeeper', 'memory', 'system', 'temp']\n",
      "['default', 'information_schema', 'main', 'pg_catalog']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "duckdb_conn = conn.get_connection(\"duckdb\")\n",
    "print(duckdb_conn.list_catalogs())\n",
    "print(duckdb_conn.list_databases())\n",
    "print(duckdb_conn.list_tables(database=\"default\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c299946d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-07 19:25:14.834\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpoor_man_lakehouse.ibis_connector.builder\u001b[0m:\u001b[36m_pyspark_connection\u001b[0m:\u001b[36m89\u001b[0m - \u001b[1mInitializing PySpark connection with Lakekeeper catalog...\u001b[0m\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create a sample dataframe\n",
    "sample_data = pl.DataFrame({\n",
    "    'id': [1, 2, 3, 4, 5],\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
    "    'value': [100, 200, 150, 300, 250]\n",
    "})\n",
    "\n",
    "spark_conn = conn.get_connection(\"pyspark\")\n",
    "\n",
    "spark_df = spark.createDataFrame(sample_data.to_pandas())\n",
    "\n",
    "spark_df.write.format(\"iceberg\").mode(\"overwrite\").saveAsTable(\"default.prova\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9decd972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing read_table for pyspark...\n",
      "   id     name  value\n",
      "0   1    Alice    100\n",
      "1   2      Bob    200\n",
      "2   3  Charlie    150\n",
      "3   4    David    300\n",
      "4   5      Eve    250\n",
      "Testing read_table for duckdb...\n",
      "   id     name  value\n",
      "0   1    Alice    100\n",
      "1   2      Bob    200\n",
      "2   3  Charlie    150\n",
      "3   4    David    300\n",
      "4   5      Eve    250\n",
      "Testing read_table for polars...\n",
      "   id     name  value\n",
      "0   1    Alice    100\n",
      "1   2      Bob    200\n",
      "2   3  Charlie    150\n",
      "3   4    David    300\n",
      "4   5      Eve    250\n"
     ]
    }
   ],
   "source": [
    "for engine in (\"pyspark\", \"duckdb\", \"polars\"):\n",
    "    print(f\"Testing read_table for {engine}...\")\n",
    "    print(conn.read_table(\"default\", \"prova\", engine).execute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52a3326d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing connection for pyspark...\n",
      "   id     name  value\n",
      "0   1    Alice    100\n",
      "1   2      Bob    200\n",
      "2   3  Charlie    150\n",
      "3   4    David    300\n",
      "4   5      Eve    250\n",
      "Testing connection for duckdb...\n",
      "   id     name  value\n",
      "0   1    Alice    100\n",
      "1   2      Bob    200\n",
      "2   3  Charlie    150\n",
      "3   4    David    300\n",
      "4   5      Eve    250\n"
     ]
    }
   ],
   "source": [
    "for engine in (\"pyspark\", \"duckdb\"):\n",
    "    print(f\"Testing connection for {engine}...\")\n",
    "    print(conn.sql(\"select * from lakekeeper.default.prova\", engine).execute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "feefe674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing read_table and Ibis syntax for pyspark...\n",
      "    name\n",
      "0    Bob\n",
      "1  David\n",
      "2    Eve\n",
      "Testing read_table and Ibis syntax for polars...\n",
      "    name\n",
      "0    Bob\n",
      "1  David\n",
      "2    Eve\n",
      "Testing read_table and Ibis syntax for duckdb...\n",
      "    name\n",
      "0    Bob\n",
      "1  David\n",
      "2    Eve\n"
     ]
    }
   ],
   "source": [
    "for engine in (\"pyspark\", \"polars\", \"duckdb\"):\n",
    "    print(f\"Testing read_table and Ibis syntax for {engine}...\")\n",
    "    t = conn.read_table(\"default\", \"prova\", engine)\n",
    "    expr = t[\"value\"] > 150 # pyright: ignore[reportOperatorIssue]\n",
    "    print(t.filter(expr).select(t.name.get_name()).execute())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d55ccfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing list tables for pyspark...\n",
      "['prova']\n",
      "Testing list tables for polars...\n",
      "['default.prova']\n",
      "Testing list tables for duckdb...\n",
      "['prova']\n"
     ]
    }
   ],
   "source": [
    "for engine in (\"pyspark\", \"polars\", \"duckdb\"):\n",
    "    print(f\"Testing list tables for {engine}...\")\n",
    "    print(conn.list_tables(engine))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poor-man-lakehouse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
